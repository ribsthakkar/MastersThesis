\chapter{Hierarchical Control for Head-to-Head Racing} 
% \epigraph{\flushright The Turn}{}
\epigraph{\flushright The second act is called ``The~ Turn.'' The magician takes the ordinary something and makes it do something extraordinary.}{Christopher Priest, \textit{The Prestige}}
\label{chapter:hier}
In this chapter, we construct our complete hierarchical controller. We explain how the discretized model from the previous chapter is used as the high-level planner and introduce two variants to serve as low-level planners. We finish the chapter by evaluating our hierarchical control variants against baseline methods representing prior works in head-to-head race simulations.
\section{Hierarchical Control Design}
% \begin{figure}
%   \centering
%   \includegraphics[height=0.5\textheight]{Figures/ControlStructureVertical.png}
%   \caption{The continuous dynamic game (top) transformed into the high-level discrete game (middle). The discrete game solution created by the high-level planner (highlighted in green) is tracked by the low-level planner (bottom).}
%   \label{fig:overall_control}
% \end{figure}
\begin{figure*}
  \centering
%   \includegraphics[width=\textwidth]{Figures/FormulationBreakdown.png}
    \includegraphics[height=0.8\textheight]{Figures/FormulationBreakdownVert.png}
  \caption[Hierarchical Control Architecture]{The uncountably infinite trajectories of the general game (top) discretized by the high-level planner (middle). The sequence of target waypoints calculated by the high-level planner (in green) is tracked by the low-level planner (bottom) and converges to a continuous trajectory (in black).}
  \label{fig:overall_control}
\end{figure*}
Traditional optimization-based control methods cannot easily be utilized for the general multi-agent racing game formulated with realistic safety and fairness rules. The rules involve nonlinear constraints over both continuous and discrete variables, and a mixed-integer nonlinear programming algorithm to solve the game would be unlikely to run at rates of \SI{15}-\SI{50}{\hertz} for precise control. This inherent challenge encourages utilizing a method such as deep reinforcement learning or trying to solve the game using short receding horizons. 

However, we propose a two-level hierarchical control design involving two parts that work to ensure the rules are followed while approximating long-term optimal choices. The high-level planner transforms the general formulation into a game with discrete states and actions where all of the discrete rules are naturally encoded. The solution provided by the high-level planner is a series of discrete states (i.e waypoints) for each player, which satisfies all of the rules. Then, the low-level planner solves a simplified version of the racing game with an objective putting greater emphasis on tracking a series of waypoints and smaller emphasis on the original game-theoretic objective and a simplified version of the rules. Therefore, this simplified game can be solved by an optimization method in real-time or be trained in a neural network when using a learning-based method. 

This method assumes if the series of waypoints produced by the high-level planner is guaranteed to follow the rules, then the control inputs generated by the waypoint tracking low-level planner will also satisfy the rules of the original game when applied to the actual underlying system. Figure \ref{fig:overall_control} visualizes the overall control architecture. 

\subsection{High-Level Planner}
The high-level planner constructs a turn-based discrete, dynamic game that is an approximation of the general game outlined in Section \ref{section:genform}. We formulate the high-level game using the model developed in Section \ref{section:discgame} and repeatedly solve it using a receding horizon of checkpoints ahead of the player for real-time control. However, our choice of horizon extends much further into the future than an MPC-based continuous state/action space controller can handle in real-time \cite{Wang2019}. In the following subsections, we briefly summarize how the discrete game is constructed and how we use Monte Carlo tree search to approximately solve the game.

\subsubsection{Discrete Game Construction}
% Describe High-Level Game (states, transitions, rewards)
Constructing our discrete game relies primarily on forming the initial states of the players. Given the initial states, the remaining feasible states and transitions of the game arise naturally by following the dynamics in Section \ref{section:discdyn}. 

To select the initial state of the game, we first construct a subset of the overall set of players based on their distances from the ego player. Because the discrete game is fixed to a finite horizon of checkpoints, the players that would likely have a significant impact on an ego player's trajectory are those who are within some local vicinity of the ego player. Using the set of local players, the initial checkpoint of the game is selected to be the one that the furthest forward player in the local vicinity has passed, and the target checkpoint is a fixed count ahead of that initial checkpoint. 

The continuous components of the players' states are projected into their discrete representations as described in Section \ref{section:discstate}. Initial state values for speed, velocity, tire wear, lane ID, and ``recent" lane changes are easily extracted from their continuous state. However, because some players may not have reached the selected initial checkpoint of the discrete game, their initial time state must be estimated. We assume every player has knowledge of the times at which each checkpoint was passed by each of the other players. The initial time state is set to 0 for the furthest forward player. Each of the remaining players' initial time states is set based on the time difference between the furthest forward player and the player. Figure \ref{fig:disc_construct} shows this initial state construction process applied to a 4-player situation from the perspective of Player 1. Player 1 ignores Player 3 in the discrete game because he is out of the range of vicinity. Within Player 1's vicinity, Player 2 is the furthest forward player, so its time state in the discrete game is 0. Player 1 sets its time state as \SI{0.2}{\second} because the last passed checkpoint of both players was checkpoint 2, and the time difference at that checkpoint was \SI{0.2}{\second}. The same logic applies in setting Player 4's initial time state. The remaining state variables are categorized in the discrete buckets based on the player's state at whatever checkpoint they may be at. This simplification may be coarse, but in practice, a player's vicinity range is relatively short. Therefore, the players' states do not change significantly within the range.

As mentioned, given the initial states of the players, the remaining construction of the game follows directly from the other parts of the formulation. The objective is the same as described in Section \ref{section:discobj}, but we do not use model checking software/algorithms to solve the game for our hierarchical controller. Table \ref{table:discrete_exp_summary} shows that it takes about 3-4 minutes to solve a single instance using state-of-the-art model checking software. We expect to run the high-level planner at \SI{1}-\SI{2}{\hertz}.

\begin{figure}
  \centering
%   \includegraphics[width=\textwidth]{Figures/FormulationBreakdown.png}
    \includegraphics[width=\textwidth]{Figures/DiscreteInitialization.png}
  \caption[High-level planner discrete game initialization]{Using the checkpoint time table (top) and the continuous states (left), Player 1 constructs the initial state of the high-level discrete game (right).}
  \label{fig:disc_construct}
\end{figure}

\subsubsection{Monte Carlo Tree Search}
Although the discrete game is much simpler than the original formulation, its state space still grows exponentially as the number of actions and players increases. Therefore, we cannot rely on synthesizing strategies using model checking methods for real-time control. To produce a reasonable strategy for the discrete game in real-time, we use the Monte Carlo tree search (MCTS) algorithm to approximate the solution to our game \cite{mcts}. Our implementation of the MCTS algorithm is the same as the original with one exception: instead of sampling the feasible action set uniformly in the simulation step, we use a heuristic to bias our sampling towards those actions deemed more likely to be realistic or optimal. Recall that the actions available at each state are pairs of lane ID and velocity in a set $A$. We sort the action set using the following characteristics of each action in the following priority:
\begin{enumerate}
    \item Ascending by the elapsed time calculation \eqref{eq:time_update}
    \item Descending by target velocity of the action
    \item Ascending by the absolute value of the difference between target lane ID and current lane ID
    \item Ascending by the difference between target lane ID and the lane ID of the optimal racing line
\end{enumerate}
Once the actions are sorted, we randomly sample an index to select an action from the sorted set using the following calculation:

\begin{equation}
    \text{action index} = [\min(|x|, |A|)], \quad
     x \sim \mathcal{N}(0, \frac{|A|}{6})
\end{equation}

Though the reward function is the same as the discrete formulation, we normalize it between 0 and 1 when calculating the upper confidence bound scoring in the MCTS algorithm. The solution produced by applying MCTS to our discrete game is a series of waypoints in the form of target lane IDs (which are mapped back to locations on track) and the target velocities at each of the locations. These waypoints are generated for all players considered in the discrete game and are selected assuming everyone selects their optimal choice. The top and middle of Figure \ref{fig:overall_control} visualize the original, continuous formulation expanded into a discrete space of lanes (in purple) and highlight target waypoints that might be calculated by MCTS (in green).  

\subsection{Low-Level Planner}
\subsubsection{Low-Level Simplified Game Formulation}
% Describe Simplified waypoint Following Game
The low-level planner is responsible for producing the control inputs, so it must operate at even higher frequencies than the high-level planner. Because we have a long-term plan in the form of target waypoints from the high-level planner, we formulate a reduced version of the original game for our low-level planner. The low-level game is formulated over a shorter horizon compared to the original game of just $\delta$ steps in $\hat{\mathcal{T}} = \{1, ..., \delta\}$. We assume that the low-level planner for player $i$ has received $k$ waypoints, $\psi^i_{r^i_{1}}, ..., \, \psi^i_{r^i_{1} + k}$, from the high-level planner. Lastly, we also refer to player $i$'s last passed checkpoint as $r^i_*$. 
% Table \ref{tab:ll_symbols} includes the additional parameters referenced in the simplified formulation.

The low-level objective involves two components. The first is to maximize the difference between its own checkpoint index and the opponents' checkpoint indices at the end of $\delta$ steps. The second is to minimize the tracking error, $\eta^i_y$, of every passed waypoint $\psi^i_{y}$. The former component influences the player to pass as many checkpoints as possible, which effectively pushes the player to reach $c_\tau$, the final checkpoint, as quickly as possible. The latter influences the player to be close to the high-level planner's target waypoints when passing each of the checkpoints. The objective also includes a multiplier $\alpha$ that balances the emphasis of the two parts. The objective of player $i$ is written as follows:
% Low-Level Formulation

\begin{equation} \label{eq:ll_obj}
    \min_{u^i_{1}, ..., u^i_{\delta}} (\sum^N_{j \neq i}r^j_{\delta} - (|N|-1) r^i_{\delta}) + \alpha \sum_{c={r^i_{1}}}^{{r^i_{1}}+k} \eta^i_c
\end{equation}

The players' continuous state dynamics, calculations for each checkpoint, and constraints on staying within track bounds \eqref{eq:ll_dyn}-\eqref{eq:ll_pos_dist} are effectively the same as the original formulation. For all players $j \in N$, the following must hold:
\begin{equation} \label{eq:ll_dyn}
    x^j_{t+1} = f(x^j_{t}, u^j_t), \quad \forall \;\; t \in \hat{\mathcal{T}}
\end{equation}
\begin{equation} \label{eq::ll_pos}
    r^j_{t+1} = p(x^j_{t+1}, r^j_t), \quad \forall \;\; t \in \hat{\mathcal{T}}
\end{equation}
\begin{equation} \label{eq::ll_pos_init}
    r^j_{1} = r^j_*
\end{equation}
\begin{equation} \label{eq:ll_pos_dist}
    q(x^m_{t}) \leq w, \quad \forall \;\; t \in \hat{\mathcal{T}}
\end{equation}

The collision avoidance rules are simplified to just maintaining a minimum separation $s_0$ as the high-level planner would have already considered the nuances of rear-end collision avoidance responsibilities outlined in \eqref{eq:gen_coll_avoid}. As a result, we require the following constraint to hold for all $t \in \hat{\mathcal{T}}$, $j \in N$, and $k \in N \setminus \{j\}$:
\begin{equation} \label{eq:ll_coll_avoid}
    d(x^j_{t}, x^k_t) \geq s_0
\end{equation}

Finally, we define the dynamics of the waypoint error, $\eta^i_y$, introduced in the objective. It is equivalent to the accumulated tracking error of each target waypoint that player $i$ has passed using a function $h: X\times X \rightarrow \mathbb{R}$ that measures the distance. If a player has not passed a waypoint, then the error variable indexed by that waypoint is set to 0. Its dynamics are expressed by the following constraint:

\begin{equation} \label{eq:ll_wp_err}
    \eta^i_y = \begin{cases} \sum_{t}^{\Hat{\mathcal{T}}} h(x^i_t, \psi^i_{c})  & \text{if } \exists \; r^i_t \geq y \\
    0 & \text{otherwise}
    \end{cases} \qquad \forall \; y \in \{r^i_{1}, ..., r^i_{1} + k\}
\end{equation}

% Low-Level params table
% \begin{table}[b]
% \centering
% \caption{Additional symbols in low-level formulation}
% \begin{tabular}{p{0.15\linewidth}|p{0.75\linewidth}}  \label{tab:ll_symbols}
% Symbol & Value  \\ 
% \hline
% $\hat{\mathcal{T}}$ & Set of steps in the game \\
% $\hat{\delta}$ & Shortened horizon \\
% $\alpha$      &   Weight parameter in objective emphasizing importance of hitting trajectory waypoints    \\
% $\psi^i_c$      &   Waypoint for player $i$ to target when passing checkpoint index $c$ \\
% $\eta^i_c$  &  Distance of player $i$'s closest approach to the waypoint $\psi^i_c$ \\ 
% $h(x^i_{t}, \psi^i_c)$  &  Function distance of player $i$'s state to waypoint $\psi^i_c$ \\  
% \end{tabular}
% \end{table}
% Describe Low-Level Formulation
This simplified formulation \eqref{eq:ll_obj}-\eqref{eq:ll_wp_err} is similar to the general formulation \eqref{eq:gen_obj}-\eqref{eq:gen_lane_lim}. However, the constraints introduced by the complex fairness and safety rules are dropped since they are considered by the high-level planner. The middle and bottom of Figure \ref{fig:overall_control} show how the target waypoints from the high-level planner (in green) are approximately tracked by a low-level planner to produce a continuous trajectory (in black). 

We consider computational two methods to solve this low-level formulation. The first method develops a reward and an observation structure to represent this simplified formulation for a multi-agent reinforcement learning (MARL) algorithm to train a policy that serves as a controller. The second method further simplifies the low-level formulation into a linear-quadratic Nash game (LQNG) to compute the control inputs. In the following subsections, we describe each of these low-level control methods.

\subsubsection{Multi-Agent Reinforcement Learning Controller}
% RL objective/reward structure for this game
Designing the MARL-based controller primarily involves shaping a reward structure that models the low-level formulation. While we provide a high-level description of the reward and penalty behaviors below, Appendix \ref{app:rl_details} includes specific details about the reward functions and when they are applied. In general, the RL agent is rewarded for the following behaviors that would improve the objective function from the low-level formulation \eqref{eq:ll_obj}:
\begin{itemize}
    \item Passing a checkpoint with an additional reward for being closer to the target lane and velocity.
    \item Minimizing the time between passing two checkpoints.
    \item Passing as many checkpoints in the limited time.
\end{itemize}
On the other hand, the agent is penalized for actions that would violate the constraints:
\begin{itemize}
    \item Swerving too frequently on straights \eqref{eq:gen_lane_lim}.
    \item Going off track or hitting a wall \eqref{eq:ll_pos_dist}.
    \item Colliding with other players \eqref{eq:ll_coll_avoid} with additional penalty if the agent is responsible for avoidance \eqref{eq:gen_coll_avoid}. 
\end{itemize}

The rewards capture our low-level formulation objective \eqref{eq:ll_obj} to pass as many checkpoints as possible while closely hitting the lane and velocity targets \eqref{eq:ll_wp_err}. The penalties capture the on-track \eqref{eq:ll_pos_dist} and collision avoidance \eqref{eq:ll_coll_avoid} constraints. Note that the penalties reintroduce the original safety and fairness from the original general game that were simplified away from the low-level formulation \eqref{eq:gen_coll_avoid} and \eqref{eq:gen_lane_lim}. Because these rules are inherently met by satisfying the objective of reaching the high-level planner's waypoints, their penalties have the weights set to have a lower impact than other components of the reward structure. We also justify incorporating the original form of these penalties because learning-based methods provide the freedom to easily encode these rules. Moreover, we are robustifying the low-level agent against the possibility that the ego player might be forced to deviate far away from the high-level plan in extreme circumstances.

\begin{figure}
  \centering
  \includegraphics[height=0.5\textwidth, angle=270]{Figures/AgentLidar.png}
  \caption[LIDAR observations of MARL-based agents.] {A bird's eye view of the LIDAR measurements captured as part of the MARL controller's observation structure.}
  \label{fig:lidar}
\end{figure}

The observation structure of the MARL controller captures the assumption of perfect information we have in all of our game formulations. Therefore, the observation structure consists of the following elements:
\begin{itemize}
    \item Perfect state information of one's own state and all other player states:
    \begin{itemize}
        \item Normalized velocity
        \item Lane ID
        \item Proportion of max allowed lane changes used
        \item Proportion of tire wear
        \item Proportion of checkpoints passed
        \item For opponent states only: 
        \begin{itemize}
            \item Position in the local coordinate frame
            \item Distance to opponent
        \end{itemize}
        \end{itemize}
    \item 9 Local LIDAR distance observations spaced over a 180\textdegree{} field of view centered in the direction that the player is facing. Figure \ref{fig:lidar} displays the LIDAR rays captured by the MARL agents in our implementation.
    \item Locations and target velocities of upcoming $k$ waypoints, $\psi^i_{r^i_{1}}, ..., \, \psi^i_{r^i_{1} + k}$, in the local coordinate frame. 
\end{itemize}


\subsubsection{Linear-Quadratic Nash Game Controller}
% Linear approximation of the waypoint following game
Our second low-level approach solves an LQNG using the coupled Riccati equations \cite{basar}. This method involves further simplifying the low-level formulation into a structure with a quadratic objective and linear dynamics. The continuous state is simplified to just four variables: $x$ position, $y$ position, $v$ velocity, and $\theta$ heading. The control inputs $u^i_t$ are also explicitly broken into acceleration, $a^i_t$, and yaw-rate, $e^i_t$. The planning horizon is reduced to $\Bar{\delta}$ where $\Bar{\delta} \ll \delta < T$. To construct our quadratic objective for player $i$, we break it into three components. The first is to minimize the squared distance to the upcoming target waypoint from the high-level planner $\Bar{\psi}^i$ calculated by the following function of some weight parameters $\rho_1, \rho_2, \text{and } \rho_3$:
\begin{multline} \label{eq:lqng_obj1}
\upsilon^i(\Bar{\psi}^i, \rho_1, \rho_2,\rho_3) =  \sum_{t = 1}^{\Bar{\delta}} (\rho_1((x^i_{t} - \Bar{\psi}^i_x)^2 + (y^i_{t} - \Bar{\psi}^i_y)^2) \\   \quad + \rho_2 (v^i_{t} - \Bar{\psi}^i_v)^2 
 + \rho_3 (\theta^i_{t} - \Bar{\psi}^i_\theta)^2)
\end{multline}

The second component is to maximize each of the other player's squared distances from the location of their estimated target waypoints $\Bar{\psi^j}$ to effectively hinder their progress. This component is calculated by the following function of the waypoint estimated target waypoint $\Bar{\psi}^j$ and a weight parameter $\rho$:
\begin{equation} \label{eq:lqng_obj2}
    \phi^i(\Bar{\psi}^j, \rho) = \sum_{t = 1}^{\Bar{\delta}} \rho((x^j_{t} - \Bar{\psi}^j_x)^2 + (y^j_{t} - \Bar{\psi}^j_y)^2)
\end{equation}

We drop all of the constraints with the exception of collision avoidance, and it is incorporated as the third component and penalty term in the objective where the distance to all other players should be maximized. This term is calculated by the following function of the opponent's position $(x^j_t, y^j_t)$ and a weight parameter $\rho$:
\begin{equation} \label{eq:lqng_obj3}
    \chi^i(x^j_t, y^j_t, \rho) = \sum_{t = 1}^{\Bar{\delta}} \rho((x^j_{t} - x^i_{t})^2 + (y^j_{t} - y^i_{t})^2)
\end{equation}

The final quadratic objective for a player  $i$ on team $\mu$ aggregates \eqref{eq:lqng_obj1}-\eqref{eq:lqng_obj3} using weight multipliers ($\rho_i$) to place varying emphasis on the components as follows:
\begin{multline} \label{eq:lqng_obj}
    \min_{a^i_{1}, e^i_{1}, ..., a^i_{\Bar{\delta}}, e^i_{\Bar{\delta}}}
    \upsilon^i(\rho_1, \rho_2,\rho_3)
    + \sum_{j \in \{\mu \setminus \{i\}\}}  (\phi^i(\Bar{\psi}^j, \rho_4)) \\
     -\sum_{j \in \{N \setminus \mu\}}  (\phi^i(\Bar{\psi}^j, \rho_5))
    - \sum_{j \in \{N \setminus \{i\}\}} (\chi^i(x^j_t, y^j_t, \rho_6))
\end{multline}

Finally, the dynamics are time invariant and linearized around initial state ($x_{t_0}$, $y_{t_0}$, $v_{t_0}$, $\theta_{t_0}$) for all players $j \in N$:
\begin{multline} \label{eq:lqng_dyn}
\begin{bmatrix}
x^j_{t+1} \\
y^j_{t+1} \\
v^j_{t+1} \\
\theta^j_{t+1} \\
\end{bmatrix} = 
\begin{bmatrix} 
	1 & 0 & \cos(\theta^j_{t_0})\Delta t & -v^j_{t_0}\sin(\theta^j_{t_0})\Delta t\\
	0 & 1 & \sin(\theta^j_{t_0})\Delta t & v^j_{t_0}\cos(\theta^j_{t_0})\Delta t\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 1\\
	\end{bmatrix}
\begin{bmatrix}
x^j_{t} \\
y^j_{t} \\
v^j_{t} \\
\theta^m_{t} \\
\end{bmatrix} \\ +
\begin{bmatrix} 
	0 & 0 \\
	0 & 0 \\
	\Delta t & 0 \\
	0 & \Delta t \\
	\end{bmatrix}
	\begin{bmatrix} 
	a^j_t  \\
	e^j_t \\
	\end{bmatrix}
\end{multline}

We use linear time-invariant dynamics for the purpose of simplicity. It is possible to estimate future states by interpolating the high-level plan to produce time-varying dynamics. However, situations may arise where a player is not near the interpolated trajectory that would require the interpolation calculations to be robust to such situations. In addition, the low-level planner runs at high frequencies, so we assume the dynamics should hold at the short horizon for which the calculations are performed.  
\subsection{Summary of Control Structure}
Algorithm \ref{alg:control_loop} outlines the main control loop of the controller. The control loop is executed at a frequency greater than or equal to that of the low-level planner. Because the rate of the high-level plan is lower than the rate of the main control loop, we just construct the initial game states and run the MCTS calculations on a background thread, asynchronously updating the target waypoints after the calculations are complete. For the low-level calculations, parallelism is not necessary because they are fast enough to meet the deadline of the main control loop. 

\begin{algorithm}
\caption{Control loop for hierarchical controller for a player $i$.}\label{alg:control_loop}
\begin{algorithmic}
\State $x^1,\ldots, x^{|N|} \gets \text{Current states of all players}$
\State $f_h \gets \text{High-level control frequency}$
\State $f_l \gets \text{Low-level control frequency}$
\State $t \gets \text{Elapsed time}$
\If{$r^i \neq c_\tau$} \Comment{Only run until we reach final checkpoint.}
\If{$t \mod 1/f_l = 0$} \Comment{Low-level control loop}
    \If{MARL Planner}
    \State $o \gets$ \Call{CollectObservations}{$x^1,\ldots, x^{|N|}$}
    \State $u^i_t \gets$ \Call{ComputeActions}{$o$}
    \ElsIf{LQNG Planner}
    \State $\Bar{\psi}^1,\ldots,\Bar{\psi}^{|N|}  \gets \psi^i_{r^i_{1}},\ldots,\psi^{|N|}_{r^{|N|}_{1}}$ 
    
    \Comment{Latest estimate of upcoming waypoint of each player.}
    
    \State $u^i_t \gets$ \Call{SolveLQNG}{$\Bar{\psi}^1,\ldots,\Bar{\psi}^{|N|}$}
    \EndIf
\EndIf
\If{$t \mod 1/f_h = 0$} \Comment{High-level control loop}
    \State $\Tilde{N} \gets$ \Call{$\text{NearbyPlayers}^i$}{$x^1,\ldots, x^{|N|}$}
    \State $\Tilde{x^1},\ldots, \Tilde{x^{|\Tilde{N}|}} \gets$ \Call{DiscreteProjection}{$\Tilde{N}$}
    \State $\psi^i_{r^i_{1}}, ..., \, \psi^i_{r^i_{1} + k} \gets$ \Call{MCTS}{$\Tilde{x^1},\ldots, \Tilde{x^{|\Tilde{N}|}}, \text{time limit}=1/f_h$} 
    
    \Comment{Run MCTS in background thread and save target waypoints.}
\EndIf
\EndIf
\end{algorithmic}
\end{algorithm}

The high-level planner is paired with each of the two low-level planners to produce two hierarchical controllers. We refer to our two hierarchical variants as MCTS-RL and MCTS-LQNG.

\section{Experimental Setup}
\subsection{Baseline Agents}
To measure the importance of our design innovations, we also consider three baseline controllers that resemble common planning and control methods developed in prior works.  
\subsubsection{End-to-End Multi-Agent Reinforcement Learning}
The end-to-end MARL controller referred to as ``E2E," represents the pure learning-based methods such as those developed by Wurman et al. and Schwarting et al. \cite{sonyai, Schwarting2021}. This controller has a similar reward/penalty structure as our low-level controller, but its observation structure is slightly different. Instead of observing the sequence of upcoming states as calculated by a high-level planner, E2E only receives the subsequence of locations from $\{c_i\}_{i=1}^{\tau}$ that denote the checkpoints at the center of the track near the agent. As a result, it is fully up to its neural networks to learn how to plan strategic and safe moves. 

\subsubsection{Fixed Trajectory Linear-Quadratic Nash Game}
The fixed trajectory LQNG controller referred to as ``Fixed-LQNG," uses the same LQNG low-level planner as our hierarchical variant, but it instead tracks a fixed trajectory around the track. This fixed trajectory is a racing line that is computed offline for a specific track using its geometry and parameters of the vehicle similar to prior works by Vazquez et al. and Stahl et al. \cite{Vazquez2020, Stahl2019_2}. However, the online tracking component involves game-theoretic reasoning by using our LQNG formulation rather than single-agent optimal control in the prior works.

\subsubsection{Fixed Trajectory Multi-Agent Reinforcement Learning}
The fixed trajectory MARL controller, referred to as ``Fixed-RL," is a learning-based counterpart to Fixed-LQNG. Control inputs are computed using a deep RL policy trained to track the precomputed target waypoints that are fixed before the race.  
\subsection{Controller Implementation}
% Implemented in Unity, used ML-Agents library for RL training
Our controllers are implemented\footnote{\codeurl} in the Unity Game Engine. Screenshots of the simulation environment are shown in Figure \ref{fig:experiment_tracks}. We extend the Karting Microgame template provided by Unity \cite{microkarting} to model our vehicles and racing environments. 

The kart physics from the template utilizes Unity's built-in physics engine by simply updating the kart's velocity and yaw rate at each control step using the acceleration and steering angle as inputs. However, to incorporate cornering limitations and tire wear modeling we modify the calculations. Tire wear proportion $e$ is modeled as an exponential decay curve that is a function of the accumulated angular velocity endured by the kart. This model captures the concept of losing grip as the tire is subjected to increased lateral loads. 

The velocity update of the kart is clamped between 0 and $\min(v_*, v_{\text{max}})$ where $v_{\text{max}}$ is the kart's maximum allowed top speed from the fixed parameters of the kart, and $v_*$ is the maximum allowed velocity based on the kart's evolved state and turning radius. The calculation for $v_*$ is used in Section \ref{section:discdyn} of the discrete game formulation to approximate feasibility of certain actions depending on the shape of the track. It is calculated by first linearly interpolating the minimum lateral acceleration $a_{\text{max}}$ and maximum lateral acceleration $a_{\text{max}}$ by the proportion of tire wear $e$ to compute the maximum allowed lateral acceleration for the kart $a_*$. Then $a_*$ is used with turning radius of the vehicle $r$ and standard formulas of circular motion to compute $v_*$. The calculations are summarized by the following pair of equations:
\begin{equation}
     a_* = a_{\text{max}} - (a_{\text{max}}-a_{\text{min}})e
\end{equation}
\begin{equation}
    v_* = \sqrt{a_*r}
\end{equation}

Multi-agent support is also added to the template to race the various autonomous controllers against each other or human players. The high-level planners run at \SI{1}{\hertz}, and low-level planners run at \SI{13}-\SI{50}{\hertz}. $\Bar{\delta}$, the planning horizon for the LQNG planner, is set to \SI{0.06}{\second} for all controllers using it. The implementation of the learning-based agents utilizes a library called Unity ML-Agents \cite{mlagents}. All of the learning-based control agents are trained using proximal policy optimization and self-play implementations from the library. They are also only trained on two sizes of oval-shaped tracks with the same number of training steps. 


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{Figures/UnityEnvironment.png}
  \caption[Screenshots of Unity simulation environment] {Kart racing environment from a racer's perspective (left), a bird's eye view of the oval track (right-top), and the complex track (right-bottom) in the Unity environment. The purple boxes visualize the lanes across checkpoints along the track, and the highlighted green boxes show planned waypoints determined by the hierarchical controllers.}
  \label{fig:experiment_tracks}
\end{figure}
\section{Head-to-Head Racing Results}
% Two Tracks, 50 races head to head each
Our experiments include head-to-head racing on a basic oval track (which the learning-based agents were trained on) and a more complex track shown in Figure \ref{fig:experiment_tracks}. Specifically, the complex track involves challenging track geometry with turns whose radii change along the curve, tight U-turns, and turns in both directions. To be successful, the optimal racing strategy requires some understanding of the shape of the track along a sequence of multiple turns. Every pair of controllers competes head-to-head in 50 races on both tracks. The dynamical parameters (cornering limitations, top speed, acceleration, braking, etc.) of each player's vehicle are identical, and the players start every race at the same initial checkpoint with the same initial tire wear of 20\%. The only difference in their initial states is the lane in which they start. To maintain fairness with respect to starting closer to the optimal racing line, we alternate the starting lanes between each race for the players.

Our experiments primarily seek to identify the importance of hierarchical game-theoretic reasoning and the strength of MCTS as a high-level planner for racing games. We count the number of wins against each opponent, average collisions-at-fault per race, average illegal lane changes per race, and a safety score (a sum of the prior two metrics) for the controllers. Appendix \ref{app:hier_results} provides a complete breakdown of all of the head-to-head results for every pair of agents. We also provide a video\footnote{\vidurl} demonstrating our controllers in action. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{Figures/HeadOvalResultsFinal.pdf}
  \caption{Results from head-to-head racing on the oval track.}
  \label{fig:results_oval}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=\textwidth]{Figures/HeadComplexResultsFinal.pdf}
  \caption{Results from head-to-head racing on the complex track.}
  \label{fig:results_complex}
\end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|p{4cm}|l|l|}
    \hline
        \textbf{Aggregate Results (400 Total Races)} & \textbf{Wins} & \textbf{Average Safety Score} \\ \hline
        MCTS-RL & 363 & 0.805 \\ \hline
        E2E & 141 & 3.23 \\ \hline
        Fixed-RL & 203 & 1.35  \\ \hline
        MCTS-LQR & 201 & 1.80 \\ \hline
        Fixed-LQR & 83 & 0.542 \\ \hline
    \end{tabular}
    \caption[Aggregated head-to-head racing results.]{Aggregated head-to-head racing results across both tracks.}
    \label{tab:aggr_results}
\end{table}
% \FloatBarrier
Based on the plots in Figure \ref{fig:results_oval} and Figure \ref{fig:results_complex} and Table \ref{tab:aggr_results}, we conclude the following key points:

\begin{enumerate}[wide, labelindent=0pt, font=\bfseries]
% MCTS-based variants outperformed respective baselines
\item \textbf{The proposed hierarchical variants outperformed their respective baselines.} 

The results amongst MCTS-RL, Fixed-RL, and E2E show the effectiveness of our hierarchical structure. While all three of the MARL-based agents were only trained on the oval track, the MCTS-RL agent won the most head-to-head races across both tracks compared to the two baselines despite being trained with the same environmental setup. Furthermore, MCTS-RL's safety score is significantly better than both E2E's and Fixed-RL's safety scores. Comparing the baselines against each other, Fixed-RL also has more wins than E2E aggregated across both tracks. This result indicates that some type of hierarchical structure is favorable. It suggests that a straightforward task of trajectory tracking is easier to learn and more effectively generalized for a deep neural network than having to learn both strategic planning and respect for the safety and fairness rules. Specifically, while learning collision avoidance was reasonably captured by all of the MARL-based agents, E2E struggled with learning the illegal lane changes rule. On the other hand, the hierarchically structured agents did not have to focus on learning such a complex rule because they rely on a provided trajectory that is assumed to follow the lane-changing rules. However, while a hierarchical structure helps with safety, it is not enough to win the races. The controller must consider multiple trajectories to be able to overtake and defend against opponents as MCTS-RL does, unlike Fixed-RL.

Next, we compare MCTS-LQNG and Fixed-LQNG. Although MCTS-LQNG has a worse overall safety score, it has 25\% more wins when aggregated over both tracks. Furthermore, the main difference in their safety score is attributed to the fact that MCTS-LQNG considers multiple trajectories whereas Fixed-LQNG only follows a fixed trajectory that does not involve changing lanes often. Therefore, it is rare for Fixed-LQNG to break the lane-changing rule. When just comparing their collision-at-fault counts, they are similar. Although on the oval track, Fixed-LQNG has a similar number of overall wins, when the racetrack is more complicated, Fixed-LQNG quickly becomes inferior. The oval track has just one main racing line, but there are many reasonable racing lines in the complex track that must be considered to be competitive. MCTS-LQNG accounts for these trajectories by using the high-level MCTS planner and is, therefore, more successful in its races against MARL-based agents on the complex track with quadruple the number of wins against them compared to the Fixed-LQNG agent. MCTS-LQNG considered trajectories that could result in overtakes when opponents made mistakes from any part of the track. On the other hand, Fixed-LQNG was forced to rely on opponents making mistakes that were not along its fixed trajectory to make overtakes. 

% RL is better than LQNG for low-level
\item \textbf{MARL is more successful and robust than LQNG as a low-level planner.}  

Overall, the MARL-based agents outperformed their LQNG-based counterparts in terms of both key metrics, wins, and safety scores. However, this result is likely due to the heuristic simplifications in our LQNG design. Vehicle dynamics are only linearized around the initial state of each agent and are time-invariant, meaning the linear approximation is only valid for a very short time horizon. Therefore, the LQNG-based agents could only rely on braking/acceleration instead of yaw-rate to avoid collisions. As a result, the weights in the objective of the LQNG formulation are set conservatively to emphasize avoiding collisions. This setup also implies that LQNG-based agents often concede in close battles and thereby lose races because of the high cost in the planning objective of driving near another player even if there is no collision. 

Due to these properties, most of their races were decided over the first few turns of the race. If the LQNG-based agent found itself ahead after the first several turns, then it usually built a considerable lead, especially Fixed-LQNG. Its opponents could not close this lead because the LQNG controller is very effective at tracking a trajectory with no obstacles. However, more often, the LQNG-based agent would fall behind or collide with an opponent at the start of the races causing it to lose speed. Once it fell behind, it could catch up to its opponents, but it could not successfully overtake unless the opponents made significant mistakes allowing it to pass with a considerable safety margin. As a result, the LQNG-based agents generally resulted in fewer wins compared to their RL-based counterparts. If the dynamics were linearized over the state at each timestep $t$ instead of just the initial state at $t_0$, we could use a larger time horizon and expect better performance for the LQNG-based agent. 

While Fixed-LQNG has a better safety score than Fixed-RL, MCTS-RL has a significantly better safety score than MCTS-LQNG. Just in terms of collision avoidance, both RL-based agents have worse numbers because the LQNG-based agents are tuned to be conservative. However, MCTS-LQNG significantly increased illegal lane changes per race compared to MCTS-RL while Fixed-LQNG has slightly fewer illegal lane changes per race compared to Fixed-RL. As discussed previously, the fixed trajectory agents do not consider alternative racing lines, so they rarely break the lane-changing limit rule in the first place. In the MCTS case, the high-level planner runs in parallel with the low-level and at a lower frequency. As a result, the calculated high-level plan uses slightly out-of-date information and does not account that the low-level controllers have already made choices that might contradict the initial steps in the plan. This mismatch causes the LQNG-based controller to more often break the lane-changing rules by swerving across the track to immediately follow the high-level plan when it is updated. The MCTS-RL is more robust to this situation because they have those safety rules encoded in their reward structures, albeit with smaller weights. They do not track the waypoints exactly and learn to smooth the trajectory produced by the high-level plan and the live situation in the game.

% MCTS-RL is the best overall controller.
\item \textbf{MCTS-RL outperforms all other implemented controllers.}  

 Aggregating the results from both tracks, MCTS-RL recorded a win rate of 83\% of the 400 head-to-head and the second-best safety score, only behind the conservatively tuned Fixed-LQNG agent. It combined the advantage of having a high-level planner that evaluates long-term plans and a low-level planner that is robust to the possibility that the high-level plans may be out of date. For example, Figure~ \ref{fig:mctsrl:overtake} demonstrates how the high-level planner provided a long-term strategy, guiding the agent to give up an advantage at present for a greater advantage in the future when overtaking. The RL-based low-level planner approximately follows the high-level strategy in case stochasticity of the MCTS algorithm yields a waypoint that seems out of place (e.g., the checkpoint between $t=3$ and $t=4$ in Figure~ \ref{fig:mctsrl:overtake}). Furthermore, MCTS-RL is also successful at executing defensive maneuvers as seen in Figure~ \ref{fig:mctsrl:defense} due to those same properties of long-term planning and low-level robustness. Both of these tactics resemble strategies of expert human drivers in real head-to-head racing.
 \end{enumerate}
 \FloatBarrier
 \begin{figure}
    \centering  \includegraphics[width=0.7\textwidth]{Figures/MCTSRLDefense.png}
  \caption[Defensive maneuver by MCTS-RL controller.]{A defensive maneuver executed by the MCTS-RL agent (green) against the E2E agent (blue) on the complex track.  Before reaching the turn, the MCTS planner calculates to switch lanes to the left first ($t=0$ to $t=1$) and then move to the right for the inside of the turn. This motion forces the E2E agent to make an evading move to avoid collision and take an even wider turn, thus increasing the overall gap at the end. The green boxes at each checkpoint highlight the long-term plan calculated by the MCTS planner for this tactic.}
  \label{fig:mctsrl:defense}
\end{figure}

 \begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textwidth]{Figures/MCTSRLOvertake.png}
  \caption [Overtaking maneuver by MCTS-RL controller.] {An overtaking maneuver executed by the MCTS-RL agent (green) against the E2E agent (blue) on the complex track. Notice how, from $t=0$ to $t=2$, the MCTS-RL agent gives up a slight advantage and takes a wider racing line on the first turn. However, the exit of the wide racing line of the first turn places the MCTS-RL agent at the inside of the next two turns where it can gain an even greater advantage when passing the E2E agent from $t=3$ to $t=6$. The green boxes at each checkpoint also highlight the long-term plan calculated by the MCTS planner for this tactic.}
  \label{fig:mctsrl:overtake}
\end{sidewaysfigure}
