\chapter{Literature Review} \label{chapter:litreview}
There are many components in the pipeline to build a complete autonomous racing system such as perception, planning, control, and hardware integration, all of which have a rapidly growing collection of literature \cite{litreview}. However, the focus of this report and literature review is limited to planning and control. Most prior work in autonomous racing control is focused on single-agent lap time optimization because multi-agent racing is an inherently more complex problem. Nevertheless, there are recent developments in multi-agent racing, but they are limited by the rules of real-life racing that are considered. They primarily only focus on basic collision avoidance. In reality, certain players bear more responsibility for collision avoidance depending on the state of the game, and there exist additional rules on lane changes to ensure safety and fairness. Finally, we also study prior works using hierarchical reasoning to solve challenging problems in several contexts of autonomous driving. 

\section{Single-Agent Racing} 
Single-agent racing approaches include both optimization and learning-based methods, which primarily focus on finding and tracking the optimal racing trajectory that minimizes lap time. \citet{Hou2016} use Monte Carlo tree search to estimate where to position the car around various shaped tracks to define an optimal trajectory. \citet{Vazquez2020} propose a method that computes an optimal trajectory offline and uses a model predictive control (MPC) algorithm to track the optimized trajectory online. Similarly, \citet{Stahl2019_2} also perform calculations offline by creating a graph representation of the track to compute a target path and use spline interpolation for smooth online path generation in an environment with static obstacles. Lastly, several studies use variants of MPC depending on the dynamics model of choice and numerical optimization methods such as sequential quadratic programming or stochastic optimization algorithms \cite{Liniger2014, Anderson2016, Kalaria2021, Kloeser2020, OKelly2020}.  

In the category of learning-based approaches, \citet{Kabzan2019} develop an online learning algorithm to update parameters of an MPC-based controller using feedback from applying control inputs. \citet{Remonda2021} use deep reinforcement learning to train a neural network policy with vehicle telemetry data. However, their experiments indicate that the method does not generalize when extended to tracks the model is not trained on. \citet{deBruin2018} use state representation to improve the generalization of the reinforcement learning-based control for racing on unseen tracks. Finally, \citet{weiss2020} develop a deep-learning framework that uses vision to estimate waypoints in a player's local space and track them using optimization-based control methods.

\section{Multi-Agent Racing}
In multi-agent racing studies, both optimization and learning-based control approaches are also used. \citet{Liniger2020} formulate and solve bimatrix games for three types of common scenarios in head-to-head racing but do not apply them to a real-time system. \citet{Li2021} use mixed-integer quadratic programming formulation with realistic collision avoidance. However, they concede that this formulation struggles to run in real-time due to the computational complexity of solving integer programs. \citet{spica2020real} propose a real-time control mechanism for a game with a pair of racing drones. This work provides an iterative-best response method while solving an MPC problem that approximates a local Nash equilibrium. It is eventually extended to automobile racing and multi-agent scenarios with more than two players by Wang et al. \cite{Wang2019, Wang2021}, but they do not consider teams. \citet{He2021} create a fast, real-time MPC algorithm to make safe overtakes, but their method does not consider adversarial behavior from the opposing players. 

Next, we outline some of the learning-based multi-agent racing works. \citet{Schwarting2021} train a policy using vision-based deep learning and self-play to predict opponent states to outperform a model that independently learns to predict opponent behaviors. Their policy produces behavior mimicking human racing drivers. \citet{Song2021} use curriculum learning to train a policy that iteratively builds its knowledge from single-agent racing to overtaking and finally to collision avoidance. Again, these approaches do not consider racing rules other than simple collision avoidance. 

\citet{sonyai} develop an autonomous racing controller using deep reinforcement learning that considers the rules of racing beyond collision avoidance. Their controller outperforms expert humans while also adhering to proper racing etiquette. It is the first study to consider nuanced safety and fairness rules of racing and does so by developing a reward structure that trains a controller to understand when it is responsible for avoiding collisions, and when it can be more aggressive. They do not encode the rules directly in their model. Instead, they refer to human experts to evaluate the behavior of their trained deep learning controllers to adjust parameters that affect the aggressiveness of their controller. However, their control design is fully learning-based and doesn't involve explicit path planning or hierarchical reasoning. In addition, although this paper models more realistic racing behavior in multi-agent racing, it also still lacks consideration of cooperative objectives amongst racing teammates. 

\section{Hierarchical Control}
Hierarchical reasoning is a method that has also been previously studied in various contexts for autonomous driving. \citet{LinigerThesis} outlines a hierarchical racing controller that constructs a high-level planner with simplified dynamics to sample sequences of constant curvature arcs and a low-level planner to use MPC to track the arc that provided the furthest progress along the track. \citet{Fisac2019} develop a two-level planning system to control an autonomous vehicle in a highway environment with aggressive human drivers. The upper-level system produces a plan to be safe against the uncertainty of the human drivers in the system by using simplified dynamics. The lower-level planner implements the strategy determined by the upper level-planner using precise dynamics. Similarly, \citet{Moghadam2019} study hierarchical reasoning for sequential decision making in highway driving. They construct a high-level planner using a trained reinforcement-learning policy to determine lane-changing plans to safely pass other drivers. The lane-changing plans are shared with low-level controllers to execute those actions. Finally, \citet{Wongpiromsarn2012} develop a reactive synthesis and hierarchical control approach where an urban driving agent's objective is defined by a series of temporal logic specifications. Their hierarchical control system has an upper level that selects target states to reach and a lower level that implements plans to reach those states in order to ensure the specifications are met. These papers have established the power of hierarchical reasoning in autonomous driving, but they have only applied it in a non-adversarial context. However, in the autonomous racing scenario, other participants in the system have competing objectives, which complicates how the hierarchical abstraction must be constructed.